{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSDS_464_Final_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzzIEcqb21XJ",
        "outputId": "e86dc6f0-ff05-452e-9d74-7e75129ae9a3"
      },
      "source": [
        "!pip install gym-chess"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-chess\n",
            "  Downloading gym_chess-0.1.1-py3-none-any.whl (27 kB)\n",
            "Collecting python-chess<0.32.0,>=0.31.1\n",
            "  Downloading python_chess-0.31.4-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym<0.18.0,>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from gym-chess) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.18.0,>=0.17.2->gym-chess) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym<0.18.0,>=0.17.2->gym-chess) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.18.0,>=0.17.2->gym-chess) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.18.0,>=0.17.2->gym-chess) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.18.0,>=0.17.2->gym-chess) (0.16.0)\n",
            "Installing collected packages: python-chess, gym-chess\n",
            "  Attempting uninstall: python-chess\n",
            "    Found existing installation: python-chess 0.23.11\n",
            "    Uninstalling python-chess-0.23.11:\n",
            "      Successfully uninstalled python-chess-0.23.11\n",
            "Successfully installed gym-chess-0.1.1 python-chess-0.31.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhAFDqL23LD4"
      },
      "source": [
        "import gym\n",
        "import gym_chess\n",
        "import random\n",
        "\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import backend"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgEc2X7F3MGY"
      },
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size, learning_rate = 0.001):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=20)\n",
        "    self.gamma = 0.95 # discount rate\n",
        "    self.epsilon = 0.7 #exploration rate\n",
        "    self.epsilon_min = 0.1\n",
        "    self.epsilon_decay = 0.99\n",
        "    self.learning_rate = learning_rate\n",
        "    self.model = self._build_model()\n",
        "    self.target_model = self._build_model()\n",
        "    self.update_target_model()\n",
        "\n",
        "  def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
        "    error = y_true - y_pred\n",
        "    cond  = backend.abs(error) <= clip_delta\n",
        "\n",
        "    squared_loss = 0.5 * backend.square(error)\n",
        "    quadratic_loss = 0.5 * backend.square(clip_delta) + clip_delta * (backend.abs(error) - clip_delta)\n",
        "\n",
        "    return backend.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
        "\n",
        "  # Neural Net for Deep-Q learning Model\n",
        "  def _build_model(self):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(units=24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(layers.Dense(units=24, activation='relu'))\n",
        "    model.add(layers.Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss=self._huber_loss, optimizer=optimizers.Adam(learning_rate=self.learning_rate), metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "  # copy weights from model to target model\n",
        "  def update_target_model(self):\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "  def memorize(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state):\n",
        "    #state = state.astype(float)\n",
        "\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      action = random.choice(env.legal_moves)\n",
        "      print('random: {}'.format(action))\n",
        "      return action\n",
        "\n",
        "    action_values = self.model.predict(state)\n",
        "    best_action = np.argmax(action_values)\n",
        "    action = action_values[best_action]\n",
        "    print('not-random: {} - {}'.format(action_values, action))\n",
        "    # return actions\n",
        "    return action\n",
        "\n",
        "  def replay(self, batch_size):\n",
        "    mini_batch = random.sample(self.memory, batch_size)\n",
        "    for state, action, reward, next_state, done in mini_batch:\n",
        "      target = self.model.predict(state)\n",
        "      if done:\n",
        "        target[0][action] = reward\n",
        "      else:\n",
        "        a = self.model.predict(next_state)[0]\n",
        "        t = self.target_model.predict(next_state)[0]\n",
        "        target[0][action] = reward + self.gamma * np.amax(t)\n",
        "      self.model.fit(state, target, epochs=1, verbose=0)\n",
        "    \n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwg-UeEFp45H"
      },
      "source": [
        "def make_matrix(board): #type(board) == chess.Board()\n",
        "  pgn = board.epd()\n",
        "  foo = []  #Final board\n",
        "  pieces = pgn.split(\" \", 1)[0]\n",
        "  rows = pieces.split(\"/\")\n",
        "\n",
        "  mapped = {\n",
        "  'P': 1,     # White Pawn\n",
        "  'p': -1,    # Black Pawn\n",
        "  'N': 2,     # White Knight\n",
        "  'n': -2,    # Black Knight\n",
        "  'B': 3,     # White Bishop\n",
        "  'b': -3,    # Black Bishop\n",
        "  'R': 4,     # White Rook\n",
        "  'r': -4,    # Black Rook\n",
        "  'Q': 5,     # White Queen\n",
        "  'q': -5,    # Black Queen\n",
        "  'K': 6,     # White King\n",
        "  'k': -6     # Black King\n",
        "  }\n",
        "    \n",
        "  for row in rows:\n",
        "    foo2 = []  #This is the row I make\n",
        "    for thing in row:\n",
        "      if thing.isdigit():\n",
        "        for i in range(0, int(thing)):\n",
        "          foo2.append(0)\n",
        "      else:\n",
        "        foo2.append(mapped[thing])\n",
        "    foo.append(foo2)\n",
        "  return np.array(foo)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "RJx6S-H3GnUi",
        "outputId": "4c619bf7-50df-4ffe-be0c-93942728f991"
      },
      "source": [
        "episode_rewards = []\n",
        "epsilon_time = []\n",
        "moving_average = []\n",
        "action_time = []\n",
        "EPISODES = 1\n",
        "\n",
        "env = gym.make('Chess-v0')\n",
        "state_size = 1\n",
        "action_size = 1\n",
        "batch_size = 64\n",
        "\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "  friendly_episode_display = episode+1\n",
        "  epsilon_time.append(agent.epsilon)\n",
        "  print('Episode {}/{}, Epsilon: {}'.format(friendly_episode_display, EPISODES, agent.epsilon))\n",
        "  \n",
        "  state = env.reset()\n",
        "  #print(state)\n",
        "  board_txt = state.fen().split()[0]\n",
        "  board_encoded = ''.join(str(ord(c)) for c in board_txt)\n",
        "  observation = [board_encoded]\n",
        "  \n",
        "  m = make_matrix(state)\n",
        "\n",
        "  history = []\n",
        "  episode_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    if state.is_checkmate() or state.is_stalemate() or state.is_insufficient_material() or state.is_game_over() or state.can_claim_threefold_repetition or state.can_claim_fifty_moves or state.can_claim_draw() or state.is_fivefold_repetition or state.is_seventyfive_moves():\n",
        "      done = True\n",
        "    \n",
        "    a = np.reshape(observation, [1, len(observation)]).astype(float)\n",
        "    #a = np.reshape(m, [1, m.shape[0]*m.shape[1]])\n",
        "    #print(a[0])\n",
        "    #action = agent.act(a[0])\n",
        "    action = agent.act(a)\n",
        "\n",
        "    if episode == EPISODES-1:\n",
        "      action_time.append(action)\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    reward = (reward*1000)-1\n",
        "\n",
        "    old_observation = observation.copy()\n",
        "    history.append(state.san(action))\n",
        "    agent.memorize(np.reshape(old_observation, [1, len(old_observation)]), action, reward, np.reshape(observation, [1, len(observation)]), done)\n",
        "    state = next_state\n",
        "\n",
        "    episode_reward+=episode_reward\n",
        "\n",
        "    if done:\n",
        "      print(env.render(mode='unicode'))\n",
        "      agent.update_target_model()\n",
        "      print('Episode {}/{}, Score: {}, Epsilon: {}'.format(friendly_episode_display, EPISODES, reward, agent.epsilon))\n",
        "      print('Episode: {}, Reward: {}'.format(episode_reward))\n",
        "      break\n",
        "    \n",
        "    if len(agent.memory) > batch_size:\n",
        "      agent.replay(batch_size)\n",
        "  episode_rewards.append(episode_reward)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1, Epsilon: 0.7\n",
            "random: a2a3\n",
            "random: d7d5\n",
            "random: d2d3\n",
            "not-random: [[nan]] - [nan]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-194-dcad282d2537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0maction_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym_chess/envs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_board\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_moves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             raise ValueError(\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"Illegal move {action} for board position {self._board.fen()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chess/__init__.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, move)\u001b[0m\n\u001b[1;32m   3552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMove\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_legal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chess/__init__.py\u001b[0m in \u001b[0;36mis_legal\u001b[0;34m(self, move)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_legal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMove\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_variant_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_pseudo_legal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_into_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_variant_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chess/__init__.py\u001b[0m in \u001b[0;36mis_pseudo_legal\u001b[0;34m(self, move)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m         \u001b[0;31m# Drops are not pseudo-legal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1615\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1616\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTnJHqh9I8Sn",
        "outputId": "7396f5bc-9da8-40ea-e837-6d19f300196c"
      },
      "source": [
        "obs = ['start']\n",
        "np.reshape(obs, [1, len(obs)])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['start']], dtype='<U5')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUtmihyi1Xzy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}